{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5343256",
   "metadata": {},
   "source": [
    "#### 1. Project Overview\n",
    "\n",
    "Goal:\n",
    "Build a relational database in PostgreSQL from the Olist Brazilian E-commerce dataset and use Python (Pandas + Plotly Express) for cleaning, feature engineering, and insights.\n",
    "\n",
    "Tools:\n",
    "\n",
    "- PostgreSQL (backend storage)\n",
    "- Python + Pandas (data cleaning, ETL)\n",
    "- SQLAlchemy (connect engine)\n",
    "- Plotly Express (visualization)\n",
    "\n",
    "Database name: `olist_db`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68f82b6",
   "metadata": {},
   "source": [
    "##### 2. Create the database in PostgreSQL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70465c08",
   "metadata": {},
   "source": [
    "In pdAdmin or the psql shell:\n",
    "\n",
    "```sql\n",
    "CREATE DATABASE olist_db;\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "To Confirm\n",
    "\n",
    "`\\l` -> list all database\n",
    "\n",
    "`\\c olist_db` -> connect to the database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e4c067",
   "metadata": {},
   "source": [
    "##### 3. Dataset files (download from kaggle)\n",
    "\n",
    "Dataset link:\n",
    "[Olist Brazilian E-Commerce dataset](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)\n",
    "\n",
    "Download and extract the csv files\n",
    "\n",
    "```sql\n",
    "olist_customers_dataset.csv\n",
    "olist_orders_dataset.csv\n",
    "olist_order_items_dataset.csv\n",
    "olist_order_payments_dataset.csv\n",
    "olist_order_reviews_dataset.csv\n",
    "olist_products_dataset.csv\n",
    "olist_sellers_dataset.csv\n",
    "olist_geolocation_dataset.csv\n",
    "product_category_name_translation.csv\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6623ce8",
   "metadata": {},
   "source": [
    "##### 4. Connect Python to PostgreSQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47628f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connected successifuly\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine,text\n",
    "import pandas as pd\n",
    "\n",
    "# create connection\n",
    "engine = create_engine(\"postgresql+psycopg2://postgres:2013%40Wewe@localhost:5432/olist_db\")\n",
    "\n",
    "print(\"connected successifuly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4995001a",
   "metadata": {},
   "source": [
    "##### 5. Load and clean data with pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4307db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded all SCV files\n"
     ]
    }
   ],
   "source": [
    "base_path = 'olist_brazil_dataset/'\n",
    "\n",
    "datasets = {\n",
    "    \"category_translation\": \"product_category_name_translation.csv\",\n",
    "    \"geolocation\": \"olist_geolocation_dataset.csv\",\n",
    "    \"sellers\": \"olist_sellers_dataset.csv\",\n",
    "    \"products\": \"olist_products_dataset.csv\",\n",
    "    \"customers\": \"olist_customers_dataset.csv\",\n",
    "    \"orders\": \"olist_orders_dataset.csv\",\n",
    "    \"order_items\": \"olist_order_items_dataset.csv\",\n",
    "    \"order_payments\": \"olist_order_payments_dataset.csv\",\n",
    "    \"order_reviews\": \"olist_order_reviews_dataset.csv\",\n",
    "}\n",
    "\n",
    "\n",
    "dfs = {name:pd.read_csv(base_path+file) for name, file in datasets.items()}\n",
    "\n",
    "print(\"loaded all SCV files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fe2498",
   "metadata": {},
   "source": [
    "##### Light cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3389f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and standardized data\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp columns in orders\n",
    "date_cols_orders =[\n",
    "    \"order_purchase_timestamp\",\n",
    "    \"order_approved_at\",\n",
    "    \"order_delivered_carrier_date\",\n",
    "    \"order_delivered_customer_date\",\n",
    "    \"order_estimated_delivery_date\",\n",
    "]\n",
    "\n",
    "for col in date_cols_orders:\n",
    "    dfs['orders'][col] = pd.to_datetime(dfs['orders'][col],errors='coerce')\n",
    "    \n",
    "# Convert timestamp columns in reviews\n",
    "date_cols_reviews = ['review_creation_date','review_answer_timestamp']\n",
    "for col in date_cols_reviews:\n",
    "    dfs['order_reviews'][col] = pd.to_datetime(dfs['order_reviews'][col],errors='coerce')\n",
    "    \n",
    "# Convert timestamp columns in order_items\n",
    "dfs[\"order_items\"][\"shipping_limit_date\"] = pd.to_datetime(\n",
    "    dfs[\"order_items\"][\"shipping_limit_date\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# handling missing values\n",
    "dfs['products'].fillna(\n",
    "    {\n",
    "        \"product_weight_g\": 0,\n",
    "        \"product_length_cm\": 0,\n",
    "        \"product_height_cm\": 0,\n",
    "        \"product_width_cm\": 0,\n",
    "    },\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "# drop duplicates just in case\n",
    "for name,df in dfs.items():\n",
    "    dfs[name] = df.drop_duplicates()\n",
    "    \n",
    "print(\"Cleaned and standardized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6a467f",
   "metadata": {},
   "source": [
    "##### Load data into postgres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fdc7cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded table :category_translation (71 rows)\n",
      "loaded table :geolocation (738,332 rows)\n",
      "loaded table :sellers (3,095 rows)\n",
      "loaded table :products (32,951 rows)\n",
      "loaded table :customers (99,441 rows)\n",
      "loaded table :orders (99,441 rows)\n",
      "loaded table :order_items (112,650 rows)\n",
      "loaded table :order_payments (103,886 rows)\n",
      "loaded table :order_reviews (99,224 rows)\n",
      "All tables successifully loaded into PostgreSQL\n"
     ]
    }
   ],
   "source": [
    "def load_table(df,tablename):\n",
    "    \"\"\"Load a single dataframe to PostgreSQL\"\"\"\n",
    "    df.to_sql(tablename,engine,if_exists='replace',index=False)\n",
    "    print(f\"loaded table :{tablename} ({len(df):,} rows)\")\n",
    "    \n",
    "for name , df in dfs.items():\n",
    "    load_table(df,name)\n",
    "\n",
    "print(\"All tables successifully loaded into PostgreSQL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb81260f",
   "metadata": {},
   "source": [
    "##### Basic checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "792dbd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orders table count : 99441\n",
      "Customers table count : 99441\n",
      "ETL process completed successifully\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM orders;\"))\n",
    "    print(\"Orders table count :\",list(result)[0][0])\n",
    "    \n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM customers\"))\n",
    "    print(\"Customers table count :\",list(result)[0][0])\n",
    "    \n",
    "print(\"ETL process completed successifully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580a9b99",
   "metadata": {},
   "source": [
    "Run these to update the constraints\n",
    "\n",
    "```sql\n",
    "\n",
    "\n",
    "-- Primary Keys\n",
    "ALTER TABLE customers ADD PRIMARY KEY (customer_id);\n",
    "ALTER TABLE sellers ADD PRIMARY KEY (seller_id);\n",
    "ALTER TABLE products ADD PRIMARY KEY (product_id);\n",
    "ALTER TABLE orders ADD PRIMARY KEY (order_id);\n",
    "ALTER TABLE category_translation ADD PRIMARY KEY (product_category_name);\n",
    "ALTER TABLE order_reviews ADD PRIMARY KEY (review_id);\n",
    "ALTER TABLE order_items ADD PRIMARY KEY (order_id, order_item_id);\n",
    "ALTER TABLE order_payments ADD PRIMARY KEY (order_id, payment_sequential);\n",
    "\n",
    "-- Foreign Keys\n",
    "ALTER TABLE orders\n",
    "  ADD CONSTRAINT fk_orders_customers FOREIGN KEY (customer_id) REFERENCES customers(customer_id);\n",
    "\n",
    "ALTER TABLE order_items\n",
    "  ADD CONSTRAINT fk_items_orders FOREIGN KEY (order_id) REFERENCES orders(order_id),\n",
    "  ADD CONSTRAINT fk_items_products FOREIGN KEY (product_id) REFERENCES products(product_id),\n",
    "  ADD CONSTRAINT fk_items_sellers FOREIGN KEY (seller_id) REFERENCES sellers(seller_id);\n",
    "\n",
    "ALTER TABLE order_payments\n",
    "  ADD CONSTRAINT fk_payments_orders FOREIGN KEY (order_id) REFERENCES orders(order_id);\n",
    "\n",
    "ALTER TABLE order_reviews\n",
    "  ADD CONSTRAINT fk_reviews_orders FOREIGN KEY (order_id) REFERENCES orders(order_id);\n",
    "\n",
    "-- sanitize the order_reviews table first\n",
    "DELETE FROM order_reviews a\n",
    "USING order_reviews b\n",
    "WHERE\n",
    "    a.ctid < b.ctid  AND\n",
    "    a.review_id = b.review_id:\n",
    "\n",
    "ALTER TABLE  order_reviews ADD PRIMARY KEY (review_id)\n",
    "\n",
    "```\n",
    "\n",
    "- this is because we need the reviews regardless of duplicates\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (virtual_env)",
   "language": "python",
   "name": "virtual_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
